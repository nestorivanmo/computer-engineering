{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifth-desktop",
   "metadata": {},
   "source": [
    "# Práctica 2 - Word2Vec\n",
    "\n",
    "- Martínez Ostoa Néstor I.\n",
    "- Procesamiento de Lenguaje Natural\n",
    "- IeC - FI - UNAM\n",
    "\n",
    "--- \n",
    "**Objetivo**: A partir del corpus seleccionado en el notebook anterior **lab-1-bpe-algorithm.ipynb** realizar un modelo de embeddings basado en Word2Vec. \n",
    "\n",
    "Pasos a realizar: \n",
    "\n",
    "1. Trabajar con el corpus tokenizado\n",
    "2. Obtener los pares de entrenamiento a partir de los contextos\n",
    "3. Construir una red neuronal con una capa con 128 unidades ocultas. Entrenar la red para obtener los embeddings\n",
    "4. Evaluar el modelo (capa de salida) con Entropía o Perplejidad\n",
    "5. Visualizar los embeddings\n",
    "6. Guardar los vectores de la capa de embedding asociados a las palabras\n",
    "\n",
    "---\n",
    "\n",
    "**Corpus elegido:** Don't Patronize Me! dataset ([link](https://github.com/Perez-AlmendrosC/dontpatronizeme))\n",
    "\n",
    "- Este corpus contiene $10,468$ párrafos extraídos de artículos de noticias con el objetivo principal de realizar un análisis para detectar lenguaje condescendiente (*patronizing and condescending language PCL*) en grupos socialmente vulnerables (refugiados, familias pobres, personas sin casa, etc)\n",
    "- Cada uno de estos párrafos están anotados con etiquetas que indican el tipo de lenguaje PCL que se encuentra en él (si es que está presente). Los párrafos se extrajeron del corpus [News on Web (NOW)](https://www.english-corpora.org/now/)\n",
    "- [Link al paper principal](https://aclanthology.org/2020.coling-main.518/)\n",
    "\n",
    "\n",
    "**Estructura del corpus (original - antes del proceso de limpieza)**\n",
    "\n",
    "- De manera general, el dataset contiene párrafos anotados con una etiqueta con valores entre $0$ y $4$ que indican el nivel de lenguaje PCL presente\n",
    "- Cada instancia del dataset está conformada de la siguiente manera:\n",
    "    - ```<doc-id>```: id del documento dentro del corpus NOW\n",
    "    - ```<keyword>```: término de búsqueda utilizado para extraer textos relacionados con una comunidad en específico\n",
    "    - ```<country-code>```: código de dos letras ISO Alpha-2\n",
    "    - ```<paragraph>```: párrafo perteneciente al ```<keyword>```\n",
    "    - ```<label>```: entero que indica el nivel de PCL presente\n",
    "    \n",
    "**Estructura del corpus actual (después del proceso de limpieza)**\n",
    "\n",
    "- ```paragraph```: párrafo limpio sin stop words, signos de puntuación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-crime",
   "metadata": {},
   "source": [
    "## 0. Bibliotecas requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "central-assets",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nestorivanmo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from utils import sigmoid, get_batches, compute_pca, get_dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-victoria",
   "metadata": {},
   "source": [
    "## 1. Tokenización del corpus\n",
    "\n",
    "- Como el corpus ya está limpio, lo único que nos queda por hacer es tokenizarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "marked-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de párrafos: 3140\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>refugees identified ioc possible contenders va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>mention moments highlight illustrate potential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>many celebrities wore blue ribbons support ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>game latest longrunning growing strategyrpg se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8107</th>\n",
       "      <td>amy fischer policy director texasbased immigra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              paragraph\n",
       "4007  refugees identified ioc possible contenders va...\n",
       "9503  mention moments highlight illustrate potential...\n",
       "6994  many celebrities wore blue ribbons support ame...\n",
       "3629  game latest longrunning growing strategyrpg se...\n",
       "8107  amy fischer policy director texasbased immigra..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dontpatronizeme_v1.4/dontpatronizeme_pcl_clean.tsv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.sample(frac=0.3, random_state=0)\n",
    "print(f\"Número de párrafos: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "increased-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[refugees, identified, ioc, possible, contende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mention, moments, highlight, illustrate, pote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[many, celebrities, wore, blue, ribbons, suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[game, latest, longrunning, growing, strategyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[amy, fischer, policy, director, texasbased, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tokenized_paragraph\n",
       "0  [refugees, identified, ioc, possible, contende...\n",
       "1  [mention, moments, highlight, illustrate, pote...\n",
       "2  [many, celebrities, wore, blue, ribbons, suppo...\n",
       "3  [game, latest, longrunning, growing, strategyr...\n",
       "4  [amy, fischer, policy, director, texasbased, i..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    tdata = nltk.word_tokenize(row[\"paragraph\"])\n",
    "    tokenized_data.append(tdata)\n",
    "    \n",
    "tokenized_df = pd.DataFrame({\"tokenized_paragraph\": tokenized_data})\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "signal-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "refugees identified ioc possible contenders various sports selection made june un refugee agency source told afp \n",
      "\n",
      "Tokenized data:\n",
      "['refugees', 'identified', 'ioc', 'possible', 'contenders', 'various', 'sports', 'selection', 'made', 'june', 'un', 'refugee', 'agency', 'source', 'told', 'afp']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original data:\\n{df.iloc[0,0]} \\n\")\n",
    "print(f\"Tokenized data:\\n{tokenized_df.iloc[0,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-above",
   "metadata": {},
   "source": [
    "## 2. Obtención de pares de entrenamiento a partir de los contextos\n",
    "\n",
    "**Entrada**: \n",
    "- DataFrame con los párrafos tokenizados\n",
    "\n",
    "**Salida**: \n",
    "- $X$: matriz de $V\\times m$ con los vectores de las palabras de contexto: ```<Pandas DataFrame>```\n",
    "- $Y$: matriz de $V \\times m$ con los vectores de las palabras centradas: ```<Pandas DataFrame>```\n",
    "\n",
    "donde $V$ es el tamaño del vocabulario de palabras del corpus y $m$ es el tamaño de la ventana y se define como $m=2c + 1$\n",
    "\n",
    "---\n",
    "**Proceso**:\n",
    "\n",
    "1. **Definir $C$**\n",
    "2. **Obtener un vocabulario del corpus**\n",
    "3. **Para cada párrafo tokenizado**:\n",
    "    - *Obtener el vector de palabras de contexto*:\n",
    "        - Con base en $C$, obtener una lista de palabras de contexto\n",
    "        - Para cada palabra de contexto, obtener su codificación *one-hot*\n",
    "        - Hacer el promedio de cada uno de los vectores de contexto\n",
    "    - *Obtener el vector de palabra de centrado*:\n",
    "        - Realizar la codificación *one-hot*\n",
    "    - *Almacenar ambos vectores en dos matrices: $X$ y $Y$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-symbol",
   "metadata": {},
   "source": [
    "### $C$ - Tamaño del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "played-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-generator",
   "metadata": {},
   "source": [
    "### Vocabulario del corpus\n",
    "\n",
    "A parte del vocabulario del corpus, obtendremos dos diccionarios útiles:\n",
    "\n",
    "1. ```word_to_index```:\n",
    "    - **Llave**: palabra del corpus\n",
    "    - **Valor**: índice numérico dentro del corpus\n",
    "    \n",
    "2. ```index_to_word```: \n",
    "    - **Llave**: índice numérico de la palabra dentro del corpus\n",
    "    - **Valor**: palabra del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "natural-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vocab(tokenized_data):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_data: <Pandas Dataframe>\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    word_vocab: <set>\n",
    "    \n",
    "    N: <int>\n",
    "        - Size of the word vocabulary\n",
    "    \"\"\"\n",
    "    word_vocab = set()\n",
    "    for _, row in tokenized_data.iterrows():\n",
    "        tokenized_paragraph = row[tokenized_data.columns[0]]\n",
    "        \n",
    "        for word in tokenized_paragraph:\n",
    "            word_vocab.add(word)\n",
    "    \n",
    "    return word_vocab, len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "central-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab, V = get_word_vocab(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "residential-salmon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:\n",
      "- 16003\n",
      "\n",
      "Vocabulary sample:\n",
      "- monitored\n",
      "- backtotheland\n",
      "- steer\n",
      "- kinky\n",
      "- detainers\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size:\\n- {V}\\n\")\n",
    "print(f\"Vocabulary sample:\")\n",
    "for w in list(word_vocab)[130:135]: print(f\"- {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "suffering-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionaries(word_vocab):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word_vocab: <set>\n",
    "        - Contains all the words in the corpus\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "        \n",
    "    index_to_word: <dictionary>\n",
    "        - Key: index of the word in the corpus\n",
    "        - Value: word\n",
    "    \"\"\"\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    \n",
    "    words = sorted(list(word_vocab))\n",
    "    for idx, word in enumerate(words):\n",
    "        index_to_word[idx] = word\n",
    "        word_to_index[word] = idx\n",
    "        \n",
    "    return word_to_index, index_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ruled-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = get_dictionaries(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-knitting",
   "metadata": {},
   "source": [
    "### Obtención de $X$ y $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "comprehensive-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_vector(word, word_to_index, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word: <str>\n",
    "    \n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "    \n",
    "    V: <int>\n",
    "        - size of the corpus' vocabulary of words\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    one_hot_vector: <Numpy's ndarray>\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word_to_index[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "def get_one_hot_from_context_words(context_words, word_to_index, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    context_words: <list>\n",
    "\n",
    "    word_to_index: <dictionary>\n",
    "    \n",
    "    V: <int>\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    one_hot_vector: <numpy's ndarray>\n",
    "        - Mean representation of all the context words' one hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_vectors = [get_one_hot_vector(w, word_to_index, V) for w in context_words]\n",
    "    return np.mean(one_hot_vectors, axis=0)\n",
    "    \n",
    "\n",
    "def get_context_centered_words(tokenized_paragraph, C):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraph: <list>\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    context_words: <list>\n",
    "    \n",
    "    centered_words: <matrix>\n",
    "    \"\"\"\n",
    "    context_words_matrix = []\n",
    "    centered_words = tokenized_paragraph\n",
    "    \n",
    "    m = len(tokenized_paragraph)\n",
    "    for idx, word in enumerate(centered_words):\n",
    "        context_words = []\n",
    "        \n",
    "        # Context words before centered word\n",
    "        if idx < C and idx != 0: context_words += tokenized_paragraph[:idx]\n",
    "        else:                    context_words += tokenized_paragraph[idx-C:idx]\n",
    "            \n",
    "        # Context words after centered word\n",
    "        if idx > m-C and idx != m-1: context_words += tokenized_paragraph[idx:]\n",
    "        else:                        context_words += tokenized_paragraph[idx+1:idx+C+1]\n",
    "            \n",
    "        context_words_matrix.append(context_words)\n",
    "    \n",
    "    return context_words_matrix, centered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "intellectual-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(tokenized_paragraphs_df, word_to_index, V, C):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraphs_df: <Pandas DataFrame>\n",
    "    \n",
    "    word_to_index: dictionary where keys are words and values are indices of the word in the corpus\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    V: <int>\n",
    "        - Size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    XY: <Pandas DataFrame>\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    centered_words_list = []\n",
    "    context_words_list = []\n",
    "    for _, row in tokenized_paragraphs_df.iterrows():\n",
    "        paragraph = row[tokenized_paragraphs_df.columns[0]]\n",
    "        context_words_matrix, centered_words = get_context_centered_words(paragraph, C)\n",
    "        \n",
    "        for idx, context_words in enumerate(context_words_matrix):\n",
    "            centered_words_list.append(centered_words[idx])\n",
    "            Y.append(\n",
    "                get_one_hot_vector(centered_words[idx], word_to_index, V)\n",
    "            )\n",
    "            \n",
    "            context_words_list.append(context_words)\n",
    "            X.append(\n",
    "                get_one_hot_from_context_words(context_words, word_to_index, V)\n",
    "            )\n",
    "    \n",
    "    df_dict = {\n",
    "        \"centered_word\": np.array(centered_words_list),\n",
    "        \"context_words\": np.array(context_words_list),\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "    }\n",
    "    XY = pd.DataFrame(df_dict)\n",
    "    return XY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "satisfied-abraham",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-51-310b6299f247>:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"context_words\": np.array(context_words_list),\n"
     ]
    }
   ],
   "source": [
    "data_df = get_X_Y(tokenized_df, word_to_index, V, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "experienced-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3140, 1)\n",
      "(75759, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>centered_word</th>\n",
       "      <th>context_words</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refugees</td>\n",
       "      <td>[identified, ioc]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>identified</td>\n",
       "      <td>[refugees, ioc, possible]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ioc</td>\n",
       "      <td>[refugees, identified, possible, contenders]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>possible</td>\n",
       "      <td>[identified, ioc, contenders, various]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contenders</td>\n",
       "      <td>[ioc, possible, various, sports]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  centered_word                                 context_words  \\\n",
       "0      refugees                             [identified, ioc]   \n",
       "1    identified                     [refugees, ioc, possible]   \n",
       "2           ioc  [refugees, identified, possible, contenders]   \n",
       "3      possible        [identified, ioc, contenders, various]   \n",
       "4    contenders              [ioc, possible, various, sports]   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                   Y  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_df.shape)\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "amino-europe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16003,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[1, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "detailed-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 75759 x 16003\n",
      "Y shape: 75759 x 16003\n"
     ]
    }
   ],
   "source": [
    "X = data_df['X']\n",
    "Y = data_df['Y']\n",
    "\n",
    "print(f\"X shape: {X.shape[0]} x {X.iloc[0].shape[0]}\")\n",
    "print(f\"Y shape: {Y.shape[0]} x {Y.iloc[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-poetry",
   "metadata": {},
   "source": [
    "## 3. Red Neuronal\n",
    "\n",
    "**Instrucción**: construir una red neuronal cocn 128 unidades ocultas. Entrenar la red para obtener los embeddings. \n",
    "\n",
    "**Desarrollo**: en esta práctica estaremos utilizando la arquitectura neuronal **word2vec** para genear embeddings de palabras. Concretamente, estaremos utilizando **CBOW (continuous bag-of-words)** como diseño de arquitectura neuronal en la cual notaremos lo siguiente: \n",
    "\n",
    "- La entrada a esta red neuronal serán los vectores de palabras de contexto $X$\n",
    "- La salida de esta red neuronal será el vector de palabras de centrado estimado $\\hat{y}$\n",
    "\n",
    "---\n",
    "\n",
    "**Arquitectura neuronal - CBOW**\n",
    "\n",
    "- **Capa de entrada $X$:**\n",
    "    - Matriz de vectores de palabras de contexto $X$\n",
    "    - $X$ es de dimensiones $V\\times m$\n",
    "        - $V=38069$\n",
    "        - $m=10672$\n",
    "- **Capa oculta $H$:**\n",
    "    - $H$ es de dimensiones $N\\times m$\n",
    "        - $N = 128$ \n",
    "    - $H = \\text{ReLU}(Z_1)$\n",
    "    - $Z_1 = W_1X + B_1 $\n",
    "- **Capa de salida $\\hat{y}$:**\n",
    "    - $\\hat{y}$ es de dimensiones $V \\times m$\n",
    "    - $\\hat{y} = \\text{softmax}(Z_2)$\n",
    "    - $Z_2 = W_2H + B_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prescribed-estonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3140, 1)\n",
      "(2198, 1)\n",
      "(942, 1)\n"
     ]
    }
   ],
   "source": [
    "tokenized_train, tokenized_test = train_test_split(tokenized_df, test_size=0.3)\n",
    "print(f\"{tokenized_df.shape}\")\n",
    "print(f\"{tokenized_train.shape}\")\n",
    "print(f\"{tokenized_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "musical-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52141 ['antimuslim', 'environment', 'encouraged', 'hindu', 'religious', 'right', 'hobnob', 'people', 'like', 'trump']\n",
      "Train vocab size: 12858\n"
     ]
    }
   ],
   "source": [
    "# Data - list of tokens\n",
    "training_data = []\n",
    "for idx, row in tokenized_train.iterrows():\n",
    "    training_data += row[\"tokenized_paragraph\"]\n",
    "print(len(data), data[:10])\n",
    "\n",
    "\n",
    "N = 128\n",
    "word2Ind, Ind2word = get_dict(training_data)\n",
    "V = len(word2Ind)\n",
    "print(f\"Train vocab size: {V}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-pasta",
   "metadata": {},
   "source": [
    "### 3.1 Funciones de activación\n",
    "\n",
    "Para implementar esta red neuronal, utilizamos dos funciones de activación:\n",
    "\n",
    "1. **Rectified Linear Unit (ReLU)**:\n",
    "    - $\\text{ReLU}(z) = \\max(0, z)$\n",
    "2. **Softmax**\n",
    "    - $\\hat{y} = \\frac{\\exp(z)}{\\sum_{j=1}^V \\exp(z_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "legitimate-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de activación\n",
    "def ReLU(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0    \n",
    "    return result\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    yhat = e_z/np.sum(e_z,axis=0)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-violin",
   "metadata": {},
   "source": [
    "### 3.2 Propagación hacía adelante\n",
    "\n",
    "1. Definición del hiperparámetro $N$\n",
    "2. Inicialización de las matrices $W_1, W_2, B_1, B_2$\n",
    "3. Funciones para calcular $Z_1, H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "humanitarian-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(N,V, random_state=1):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    N:  <int>\n",
    "        Dimension of the hidden layer\n",
    "        \n",
    "    V:  <int>\n",
    "        Dimension of vocabulary\n",
    "        \n",
    "    random_state: <int>\n",
    "        Make random results consistent - could be any number\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    W1: <numpy's ndarray>\n",
    "        - Weights of the hidden layer of dimensions NxV\n",
    "        \n",
    "    b1: <numpy's ndarray>\n",
    "        - Biases of the hidden layer of dimensions Nx1\n",
    "        \n",
    "    W2: <numpy's ndarray>\n",
    "        - Weights of the output layer of dimensions VxN\n",
    "        \n",
    "    b2: <numpy's ndarray>\n",
    "        - Biases of the output layer of dimensions Vx1\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    W1 = np.random.rand(N, V)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    \n",
    "    W2 = np.random.rand(V, N)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def forward_propagation(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    x: <numpy's ndarray>\n",
    "        - Average One hot encoded vector\n",
    "        \n",
    "    W1: <numpy's ndarray>\n",
    "        - Weights of the hidden layer of dimensions NxV\n",
    "        \n",
    "    b1: <numpy's ndarray>\n",
    "        - Biases of the hidden layer of dimensions Nx1\n",
    "        \n",
    "    W2: <numpy's ndarray>\n",
    "        - Weights of the output layer of dimensions VxN\n",
    "        \n",
    "    b2: <numpy's ndarray>\n",
    "        - Biases of the output layer of dimensions Vx1\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    z: <numpy's ndarray>\n",
    "        - Score vector\n",
    "    \"\"\"\n",
    "    \n",
    "    h = np.dot(W1,x)+b1\n",
    "    h = np.maximum(0,h)\n",
    "    z = np.dot(W2,h)+b2\n",
    "    \n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-shape",
   "metadata": {},
   "source": [
    "### 3.3 Función de costo\n",
    "\n",
    "Para esta red neuronal utilizaremos la función pérdida entropía cruzada la cual se define como: \n",
    "\n",
    "$$ J=-\\sum_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "invalid-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    \"\"\"\n",
    "    Cost using Cross Entropy Loss\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    y: <numpy's ndarray>\n",
    "        - Original vector (of centered words)\n",
    "    \n",
    "    yhat: <numpy's ndarray>\n",
    "        - Predicted vector\n",
    "    \n",
    "    batch_size: <int>\n",
    "        - Indicates the amount of training and real values to take on each batch\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cost: <double>\n",
    "    \"\"\"\n",
    "    \n",
    "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-flashing",
   "metadata": {},
   "source": [
    "### 3.4 Retropropagación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "palestinian-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, yhat, y, h, W1, b1, W2, b2, batch_size):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    x: <numpy's ndarray>\n",
    "        - Average one hot encoded vector (context words)\n",
    "        \n",
    "    yhat: <numpy's ndarray>\n",
    "        - Predicted vector (centered word)\n",
    "        \n",
    "    y: <numpy's ndarray>\n",
    "        - Target vector\n",
    "        \n",
    "    h: <numpy's ndarray>\n",
    "        - Hidden vector\n",
    "        \n",
    "    W1: <numpy's ndarray>\n",
    "        - Weights of the hidden layer of dimensions NxV\n",
    "        \n",
    "    b1: <numpy's ndarray>\n",
    "        - Biases of the hidden layer of dimensions Nx1\n",
    "        \n",
    "    W2: <numpy's ndarray>\n",
    "        - Weights of the output layer of dimensions VxN\n",
    "        \n",
    "    b2: <numpy's ndarray>\n",
    "        - Biases of the output layer of dimensions Vx1\n",
    "    \n",
    "    batch_size: <int>\n",
    "        - Indicates the amount of training and real values to take on each batch\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    W1_grad: <numpy's ndarray>\n",
    "        - Weights of the gradient's hidden layer of dimensions NxV\n",
    "        \n",
    "    b1_grad: <numpy's ndarray>\n",
    "        - Biases of the gradient's hidden layer of dimensions Nx1\n",
    "        \n",
    "    W2_grad: <numpy's ndarray>\n",
    "        - Weights of the gradient's output layer of dimensions VxN\n",
    "        \n",
    "    b2_grad: <numpy's ndarray>\n",
    "        - Biases of the gradient's output layer of dimensions Vx1\n",
    "    \"\"\"\n",
    "    \n",
    "    l1 = np.dot(W2.T,(yhat-y))\n",
    "    l1 = np.maximum(0,l1) #ReLU\n",
    "    \n",
    "    grad_W1 = (1/batch_size)*np.dot(l1,x.T)    #1/m * relu(w2.T(yhat-y)) . xT\n",
    "    grad_b1 = np.sum((1/batch_size)*np.dot(l1,x.T),axis=1,keepdims=True)\n",
    "    grad_W2 = (1/batch_size)*np.dot(yhat-y,h.T)\n",
    "    grad_b2 = np.sum((1/batch_size)*np.dot(yhat-y,h.T),axis=1,keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_b1, grad_W2, grad_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-dylan",
   "metadata": {},
   "source": [
    "### 3.5 Descenso del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "forty-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, C, num_iters, alpha=0.03):\n",
    "    \"\"\"\n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases   \n",
    "\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = init_model(N, V, random_state=0)\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    C = 2\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        z, h = forward_propagation(x, W1, b1, W2, b2)\n",
    "        yhat = softmax(z)\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        \n",
    "        if ((iters+1) % 10 == 0):\n",
    "            print(f\"Number of iterations: {iters + 1} => cost: {cost:.6f}\")\n",
    "            \n",
    "        grad_W1, grad_b1, grad_W2, grad_b2 = backward_propagation(\n",
    "            x, yhat, y, h, W1, b1, W2, b2, batch_size\n",
    "        )\n",
    "        \n",
    "        W1 -= alpha*grad_W1 \n",
    "        W2 -= alpha*grad_W2\n",
    "        b1 -= alpha*grad_b1\n",
    "        b2 -= alpha*grad_b2\n",
    "        \n",
    "        iters += 1 \n",
    "        if iters == num_iters:  break\n",
    "        if iters % 100 == 0: alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-fireplace",
   "metadata": {},
   "source": [
    "### 3.6 Entrenamiendo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "curious-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 10 cost: 0.008509\n",
      "Number of iterations: 20 cost: 0.006306\n",
      "Number of iterations: 30 cost: 0.005012\n",
      "Number of iterations: 40 cost: 0.004160\n",
      "Number of iterations: 50 cost: 0.003556\n",
      "Number of iterations: 60 cost: 0.003106\n",
      "Number of iterations: 70 cost: 0.002757\n",
      "Number of iterations: 80 cost: 0.002479\n",
      "Number of iterations: 90 cost: 0.002252\n",
      "Number of iterations: 100 cost: 0.002063\n",
      "Number of iterations: 110 cost: 0.001950\n",
      "Number of iterations: 120 cost: 0.001853\n",
      "Number of iterations: 130 cost: 0.001765\n",
      "Number of iterations: 140 cost: 0.001686\n",
      "Number of iterations: 150 cost: 0.001613\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "N = 128\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "W1, W2, b1, b2 = gradient_descent(training_data, word2Ind, N, V, C, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-philippines",
   "metadata": {},
   "source": [
    "## 4. Evaluación del modelo\n",
    "\n",
    "Para evaluar el modelo primero obtendremos los embeddings de la última capa de salida de la red neuronal. Para eso, utilizaremos el promedio de los pesos de las matrices $W_1$ y $W_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "promotional-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test vocab size: 12858\n"
     ]
    }
   ],
   "source": [
    "# Data - list of tokens\n",
    "test_data = []\n",
    "for idx, row in tokenized_test.iterrows():\n",
    "    test_data += row[\"tokenized_paragraph\"]\n",
    "word2Ind_test, Ind2word_test = get_dict(test_data)\n",
    "V_test = len(word2Ind_test)\n",
    "print(f\"Test vocab size: {V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "promotional-walnut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings dimensions: (12858, 128)\n"
     ]
    }
   ],
   "source": [
    "embeddings = (W1.T + W2) / 2.0\n",
    "print(f\"Embeddings dimensions: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-liechtenstein",
   "metadata": {},
   "source": [
    "## 5. Visualización de los Word Embeddings\n",
    "\n",
    "1. Obtenemos los embeddings\n",
    "2. Aplicamos técnicas de reducción de dimensionalidad (PCA por ejemplo)\n",
    "3. Normalización de los datos\n",
    "4. Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "continuous-pittsburgh",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46769</th>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17533</th>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>libby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17042</th>\n",
       "      <td>border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41133</th>\n",
       "      <td>controversial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>debuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48601</th>\n",
       "      <td>useless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21645</th>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40472</th>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25916</th>\n",
       "      <td>track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46011</th>\n",
       "      <td>elections</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29715</th>\n",
       "      <td>came</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26396</th>\n",
       "      <td>edward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41298</th>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37521</th>\n",
       "      <td>wearing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word\n",
       "46769           many\n",
       "17533         system\n",
       "8176           libby\n",
       "17042         border\n",
       "41133  controversial\n",
       "5460          debuts\n",
       "48601        useless\n",
       "21645          money\n",
       "40472           line\n",
       "25916          track\n",
       "46011      elections\n",
       "29715           came\n",
       "26396         edward\n",
       "41298          great\n",
       "37521        wearing"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.DataFrame({'word': training_data})\n",
    "words_df = words_df.sample(frac=0.0005)\n",
    "words = words_df.values\n",
    "print(f\"Words: {len(words)}\")\n",
    "words_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "assured-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Embeddings\n",
    "embeddings = (W1.T + W2) / 2.0\n",
    "idx = [word2Ind[word[0]] for word in words]\n",
    "X = embeddings[idx, :]\n",
    "\n",
    "# 2. PCA\n",
    "X = PCA(n_components=3).fit_transform(X)\n",
    "\n",
    "# 3. Normalización\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "level-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plotly.com"
       },
       "data": [
        {
         "hovertemplate": "X=%{x}<br>Y=%{y}<br>Z=%{z}<br>word=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "size": 5,
          "symbol": "circle"
         },
         "mode": "markers+text",
         "name": "",
         "scene": "scene",
         "showlegend": false,
         "text": [
          "many",
          "system",
          "libby",
          "border",
          "controversial",
          "debuts",
          "useless",
          "money",
          "line",
          "track",
          "elections",
          "came",
          "edward",
          "great",
          "wearing",
          "housing",
          "others",
          "distant",
          "system",
          "regulations",
          "fearing",
          "thousands",
          "infrastructure",
          "still",
          "although",
          "among"
         ],
         "textposition": "top center",
         "type": "scatter3d",
         "x": [
          -0.09178534674920079,
          3.0419692064082478,
          -1.3123963854449028,
          -0.3252609760688696,
          -0.7609389666720555,
          0.5146668306257799,
          0.24547324214517569,
          -0.06904675809043835,
          -0.4061873701134205,
          -0.6780038782732549,
          -0.47294636250918853,
          0.028336589260403916,
          -0.43220031195450337,
          -0.5919896962424068,
          0.08153840914477561,
          0.8702203577990404,
          -0.7441459471085854,
          -0.9482046818146483,
          3.041969206408247,
          -0.6991561338491978,
          0.12746483594911878,
          0.03250309047624531,
          -0.46258235574888984,
          -0.3296766873969583,
          0.528939391716068,
          -0.18855930189658127
         ],
         "y": [
          1.139086471882371,
          -0.0025950287997956595,
          -0.2611958779105655,
          -1.6216884441234178,
          0.7531601987624369,
          0.07368725919726421,
          -0.11490830825341244,
          1.009520645025284,
          -0.5165537244081192,
          1.4942195820825177,
          -1.5785538865252653,
          0.8522306459555974,
          -1.591116815003773,
          -0.25240165547112986,
          -0.34172195935382954,
          -0.8145642071516507,
          -0.46559881682893584,
          0.34587009168621097,
          -0.002595028799794491,
          0.8982053865746518,
          -0.08557601546687964,
          -0.017933284095505736,
          2.1131053054067284,
          -1.948369728345829,
          1.3212429002842376,
          -0.38495570631939524
         ],
         "z": [
          -1.4553643425980074,
          -0.6283397665568629,
          -1.449971641020924,
          -0.1370757474249552,
          -1.4071958508788345,
          0.002911694177016337,
          0.7584392288593386,
          0.31272165928136436,
          0.43267771651034853,
          -1.972494739642078,
          -0.023137973019696046,
          0.35667784569161665,
          0.4162582128861972,
          0.5874850906831343,
          -0.3555125271482754,
          0.2678526112128051,
          1.6995520940290094,
          -0.8497742434522393,
          -0.6283397665568633,
          0.6230384056480002,
          -0.020426780574988902,
          2.17978948187052,
          1.0263687087298132,
          -1.4237076997508575,
          1.1089430106281877,
          0.5786253184172313
         ]
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "scene": {
         "domain": {
          "x": [
           0,
           1
          ],
          "y": [
           0,
           1
          ]
         },
         "xaxis": {
          "title": {
           "text": "X"
          }
         },
         "yaxis": {
          "title": {
           "text": "Y"
          }
         },
         "zaxis": {
          "title": {
           "text": "Z"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"781bae3b-a96b-42e5-b719-de0bff29f978\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"781bae3b-a96b-42e5-b719-de0bff29f978\")) {                    Plotly.newPlot(                        \"781bae3b-a96b-42e5-b719-de0bff29f978\",                        [{\"hovertemplate\": \"X=%{x}<br>Y=%{y}<br>Z=%{z}<br>word=%{text}<extra></extra>\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"size\": 5, \"symbol\": \"circle\"}, \"mode\": \"markers+text\", \"name\": \"\", \"scene\": \"scene\", \"showlegend\": false, \"text\": [\"many\", \"system\", \"libby\", \"border\", \"controversial\", \"debuts\", \"useless\", \"money\", \"line\", \"track\", \"elections\", \"came\", \"edward\", \"great\", \"wearing\", \"housing\", \"others\", \"distant\", \"system\", \"regulations\", \"fearing\", \"thousands\", \"infrastructure\", \"still\", \"although\", \"among\"], \"textposition\": \"top center\", \"type\": \"scatter3d\", \"x\": [-0.09178534674920079, 3.0419692064082478, -1.3123963854449028, -0.3252609760688696, -0.7609389666720555, 0.5146668306257799, 0.24547324214517569, -0.06904675809043835, -0.4061873701134205, -0.6780038782732549, -0.47294636250918853, 0.028336589260403916, -0.43220031195450337, -0.5919896962424068, 0.08153840914477561, 0.8702203577990404, -0.7441459471085854, -0.9482046818146483, 3.041969206408247, -0.6991561338491978, 0.12746483594911878, 0.03250309047624531, -0.46258235574888984, -0.3296766873969583, 0.528939391716068, -0.18855930189658127], \"y\": [1.139086471882371, -0.0025950287997956595, -0.2611958779105655, -1.6216884441234178, 0.7531601987624369, 0.07368725919726421, -0.11490830825341244, 1.009520645025284, -0.5165537244081192, 1.4942195820825177, -1.5785538865252653, 0.8522306459555974, -1.591116815003773, -0.25240165547112986, -0.34172195935382954, -0.8145642071516507, -0.46559881682893584, 0.34587009168621097, -0.002595028799794491, 0.8982053865746518, -0.08557601546687964, -0.017933284095505736, 2.1131053054067284, -1.948369728345829, 1.3212429002842376, -0.38495570631939524], \"z\": [-1.4553643425980074, -0.6283397665568629, -1.449971641020924, -0.1370757474249552, -1.4071958508788345, 0.002911694177016337, 0.7584392288593386, 0.31272165928136436, 0.43267771651034853, -1.972494739642078, -0.023137973019696046, 0.35667784569161665, 0.4162582128861972, 0.5874850906831343, -0.3555125271482754, 0.2678526112128051, 1.6995520940290094, -0.8497742434522393, -0.6283397665568633, 0.6230384056480002, -0.020426780574988902, 2.17978948187052, 1.0263687087298132, -1.4237076997508575, 1.1089430106281877, 0.5786253184172313]}],                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"scene\": {\"domain\": {\"x\": [0.0, 1.0], \"y\": [0.0, 1.0]}, \"xaxis\": {\"title\": {\"text\": \"X\"}}, \"yaxis\": {\"title\": {\"text\": \"Y\"}}, \"zaxis\": {\"title\": {\"text\": \"Z\"}}}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('781bae3b-a96b-42e5-b719-de0bff29f978');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Visualización\n",
    "embeddings_df = pd.DataFrame(data=X, columns=['X', 'Y', 'Z'])\n",
    "embeddings_df['word'] = [word[0] for word in words]\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    embeddings_df, x='X', y='Y', z='Z', text=\"word\"\n",
    ")\n",
    "fig.update_traces(\n",
    "    marker=dict(size=5), textposition='top center'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-vienna",
   "metadata": {},
   "source": [
    "## 6. Almacenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "stone-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12858, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aap</td>\n",
       "      <td>[0.7244269791785534, 0.8359355001269977, 0.700...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ababa</td>\n",
       "      <td>[0.7745357827248547, 0.765039999143007, 0.7689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abadies</td>\n",
       "      <td>[0.7825410524052706, 0.313356671072614, 0.5257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandon</td>\n",
       "      <td>[0.48739499506886336, 0.493720983620092, 0.514...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>[0.46609545188343415, 0.4776234384153497, 0.67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                                          embedding\n",
       "0        aap  [0.7244269791785534, 0.8359355001269977, 0.700...\n",
       "1      ababa  [0.7745357827248547, 0.765039999143007, 0.7689...\n",
       "2    abadies  [0.7825410524052706, 0.313356671072614, 0.5257...\n",
       "3    abandon  [0.48739499506886336, 0.493720983620092, 0.514...\n",
       "4  abandoned  [0.46609545188343415, 0.4776234384153497, 0.67..."
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = (W1.T + W2) / 2.0\n",
    "embeddings = [e for e in embeddings]\n",
    "\n",
    "embeddings_df = pd.DataFrame({'word':list(word2Ind.keys()), \n",
    "                              'embedding': embeddings})\n",
    "print(embeddings_df.shape)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "severe-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df.to_csv('embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-catalog",
   "metadata": {},
   "source": [
    "**Referencias**:\n",
    "\n",
    "\n",
    "- [V. Mijangos - Curso Procesamiento de Lenguaje Natural](https://github.com/VMijangos/Curso-Procesamiento-de-Lenguaje-Natural/tree/master/Notebooks)\n",
    "- [ElotlMX - Curso Redes Neuronales](https://github.com/ElotlMX/Curso_redes)\n",
    "- [Deep Learning AI NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-scottish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
