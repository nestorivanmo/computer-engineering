{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifth-desktop",
   "metadata": {},
   "source": [
    "# Práctica 2 - Word2Vec\n",
    "\n",
    "- Martínez Ostoa Néstor I.\n",
    "- Procesamiento de Lenguaje Natural\n",
    "- IeC - FI - UNAM\n",
    "\n",
    "--- \n",
    "**Objetivo**: A partir del corpus seleccionado en el notebook anterior **lab-1-bpe-algorithm.ipynb** realizar un modelo de embeddings basado en Word2Vec. \n",
    "\n",
    "Pasos a realizar: \n",
    "\n",
    "1. Trabajar con el corpus tokenizado\n",
    "2. Obtener los pares de entrenamiento a partir de los contextos\n",
    "3. Construir una red neuronal con una capa con 128 unidades ocultas. Entrenar la red para obtener los embeddings\n",
    "4. Evaluar el modelo (capa de salida) con Entropía o Perplejidad\n",
    "5. Visualizar los embeddings\n",
    "6. Guardar los vectores de la capa de embedding asociados a las palabras\n",
    "\n",
    "---\n",
    "\n",
    "**Corpus elegido:** Don't Patronize Me! dataset ([link](https://github.com/Perez-AlmendrosC/dontpatronizeme))\n",
    "\n",
    "- Este corpus contiene $10,468$ párrafos extraídos de artículos de noticias con el objetivo principal de realizar un análisis para detectar lenguaje condescendiente (*patronizing and condescending language PCL*) en grupos socialmente vulnerables (refugiados, familias pobres, personas sin casa, etc)\n",
    "- Cada uno de estos párrafos están anotados con etiquetas que indican el tipo de lenguaje PCL que se encuentra en él (si es que está presente). Los párrafos se extrajeron del corpus [News on Web (NOW)](https://www.english-corpora.org/now/)\n",
    "- [Link al paper principal](https://aclanthology.org/2020.coling-main.518/)\n",
    "\n",
    "\n",
    "**Estructura del corpus (original - antes del proceso de limpieza)**\n",
    "\n",
    "- De manera general, el dataset contiene párrafos anotados con una etiqueta con valores entre $0$ y $4$ que indican el nivel de lenguaje PCL presente\n",
    "- Cada instancia del dataset está conformada de la siguiente manera:\n",
    "    - ```<doc-id>```: id del documento dentro del corpus NOW\n",
    "    - ```<keyword>```: término de búsqueda utilizado para extraer textos relacionados con una comunidad en específico\n",
    "    - ```<country-code>```: código de dos letras ISO Alpha-2\n",
    "    - ```<paragraph>```: párrafo perteneciente al ```<keyword>```\n",
    "    - ```<label>```: entero que indica el nivel de PCL presente\n",
    "    \n",
    "**Estructura del corpus actual (después del proceso de limpieza)**\n",
    "\n",
    "- ```paragraph```: párrafo limpio sin stop words, signos de puntuación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-crime",
   "metadata": {},
   "source": [
    "## Bibliotecas requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "central-assets",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nestorivanmo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-victoria",
   "metadata": {},
   "source": [
    "## Tokenización del corpus\n",
    "\n",
    "- Como el corpus ya está limpio, lo único que nos queda por hacer es tokenizarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "marked-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de párrafos: 10468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>living times absolute insanity pretty sure peo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>libya today countless number ghanaian nigerian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white house press secretary sean spicer said f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>council customers signs would displayed two sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like received migrants fleeing el salvador gua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paragraph\n",
       "0  living times absolute insanity pretty sure peo...\n",
       "1  libya today countless number ghanaian nigerian...\n",
       "2  white house press secretary sean spicer said f...\n",
       "3  council customers signs would displayed two sp...\n",
       "4  like received migrants fleeing el salvador gua..."
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dontpatronizeme_v1.4/dontpatronizeme_pcl_clean.tsv\"\n",
    "df = pd.read_csv(path)\n",
    "print(f\"Número de párrafos: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "increased-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[living, times, absolute, insanity, pretty, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[libya, today, countless, number, ghanaian, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[white, house, press, secretary, sean, spicer,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[council, customers, signs, would, displayed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[like, received, migrants, fleeing, el, salvad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tokenized_paragraph\n",
       "0  [living, times, absolute, insanity, pretty, su...\n",
       "1  [libya, today, countless, number, ghanaian, ni...\n",
       "2  [white, house, press, secretary, sean, spicer,...\n",
       "3  [council, customers, signs, would, displayed, ...\n",
       "4  [like, received, migrants, fleeing, el, salvad..."
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    tdata = nltk.word_tokenize(row[\"paragraph\"])\n",
    "    tokenized_data.append(tdata)\n",
    "    \n",
    "tokenized_df = pd.DataFrame({\"tokenized_paragraph\": tokenized_data})\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "signal-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "living times absolute insanity pretty sure people aware waking every day check news seemed carry feeling panic dread action heroes probably face trying decide whether cut blue green wire ticking bomb except bomb instructions long ago burned fire imminent catastrophe seems likeliest outcome hard stay onedge long though natural people become inured constant chaos slump malaise hopelessness pessimism \n",
      "\n",
      "Tokenized data:\n",
      "['living', 'times', 'absolute', 'insanity', 'pretty', 'sure', 'people', 'aware', 'waking', 'every', 'day', 'check', 'news', 'seemed', 'carry', 'feeling', 'panic', 'dread', 'action', 'heroes', 'probably', 'face', 'trying', 'decide', 'whether', 'cut', 'blue', 'green', 'wire', 'ticking', 'bomb', 'except', 'bomb', 'instructions', 'long', 'ago', 'burned', 'fire', 'imminent', 'catastrophe', 'seems', 'likeliest', 'outcome', 'hard', 'stay', 'onedge', 'long', 'though', 'natural', 'people', 'become', 'inured', 'constant', 'chaos', 'slump', 'malaise', 'hopelessness', 'pessimism']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original data:\\n{df.iloc[0,0]} \\n\")\n",
    "print(f\"Tokenized data:\\n{tokenized_df.iloc[0,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-above",
   "metadata": {},
   "source": [
    "## Obtención de pares de entrenamiento a partir de los contextos\n",
    "\n",
    "**Entrada**: \n",
    "- DataFrame con los párrafos tokenizados\n",
    "\n",
    "**Salida**: \n",
    "- $X$: matriz de $V\\times m$ con los vectores de las palabras de contexto: ```<Pandas DataFrame>```\n",
    "- $Y$: matriz de $V \\times m$ con los vectores de las palabras centradas: ```<Pandas DataFrame>```\n",
    "\n",
    "donde $V$ es el tamaño del vocabulario de palabras del corpus y $m$ es el tamaño de la ventana y se define como $m=2c + 1$\n",
    "\n",
    "---\n",
    "**Proceso**:\n",
    "\n",
    "1. **Definir $C$**\n",
    "2. **Obtener un vocabulario del corpus**\n",
    "3. **Para cada párrafo tokenizado**:\n",
    "    - *Obtener el vector de palabras de contexto*:\n",
    "        - Con base en $C$, obtener una lista de palabras de contexto\n",
    "        - Para cada palabra de contexto, obtener su codificación *one-hot*\n",
    "        - Hacer el promedio de cada uno de los vectores de contexto\n",
    "    - *Obtener el vector de palabra de centrado*:\n",
    "        - Realizar la codificación *one-hot*\n",
    "    - *Almacenar ambos vectores en dos matrices: $X$ y $Y$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-symbol",
   "metadata": {},
   "source": [
    "### $C$ - Tamaño del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "played-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-generator",
   "metadata": {},
   "source": [
    "### Vocabulario del corpus\n",
    "\n",
    "A parte del vocabulario del corpus, obtendremos dos diccionarios útiles:\n",
    "\n",
    "1. ```word_to_index```:\n",
    "    - **Llave**: palabra del corpus\n",
    "    - **Valor**: índice numérico dentro del corpus\n",
    "    \n",
    "2. ```index_to_word```: \n",
    "    - **Llave**: índice numérico de la palabra dentro del corpus\n",
    "    - **Valor**: palabra del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "natural-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vocab(tokenized_data):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_data: <Pandas Dataframe>\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    word_vocab: <set>\n",
    "    \n",
    "    N: <int>\n",
    "        - Size of the word vocabulary\n",
    "    \"\"\"\n",
    "    word_vocab = set()\n",
    "    for _, row in tokenized_data.iterrows():\n",
    "        tokenized_paragraph = row[tokenized_data.columns[0]]\n",
    "        \n",
    "        for word in tokenized_paragraph:\n",
    "            word_vocab.add(word)\n",
    "    \n",
    "    return word_vocab, len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "central-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab, V = get_word_vocab(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "residential-salmon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:\n",
      "- 30732\n",
      "\n",
      "Vocabulary sample:\n",
      "- conflicts\n",
      "- leagues\n",
      "- pcg\n",
      "- subtext\n",
      "- gambling\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size:\\n- {V}\\n\")\n",
    "print(f\"Vocabulary sample:\")\n",
    "for w in list(word_vocab)[130:135]: print(f\"- {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "suffering-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionaries(word_vocab):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word_vocab: <set>\n",
    "        - Contains all the words in the corpus\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "        \n",
    "    index_to_word: <dictionary>\n",
    "        - Key: index of the word in the corpus\n",
    "        - Value: word\n",
    "    \"\"\"\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    \n",
    "    words = sorted(list(word_vocab))\n",
    "    for idx, word in enumerate(words):\n",
    "        index_to_word[idx] = word\n",
    "        word_to_index[word] = idx\n",
    "        \n",
    "    return word_to_index, index_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ruled-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = get_dictionaries(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "future-peripheral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index: 123\n",
      "index_to_word: abstain\n"
     ]
    }
   ],
   "source": [
    "print(f\"word_to_index: {word_to_index['abstain']}\")\n",
    "print(f\"index_to_word: {index_to_word[123]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-knitting",
   "metadata": {},
   "source": [
    "### Obtención de $X$ y $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "comprehensive-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_vector(word, word_to_index, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word: <str>\n",
    "    \n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "    \n",
    "    V: <int>\n",
    "        - size of the corpus' vocabulary of words\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    oh_vector: <Numpy's ndarray>\n",
    "    \"\"\"\n",
    "    oh_vector = np.zeros(V)\n",
    "    oh_vector[word_to_index[word]] = 1\n",
    "    \n",
    "    return oh_vector\n",
    "\n",
    "def get_one_hot_from_context_words(context_words, word_to_index, V):\n",
    "    context_words_vectors = [get_one_hot_vector(w, word_to_index, V) for w in context_words]\n",
    "    return np.mean(context_words_vectors, axis=0)\n",
    "    \n",
    "\n",
    "def get_context_centered_words(tokenized_paragraph, C):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraph: <list>\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    context_words: <list>\n",
    "    \n",
    "    centered_words: <matrix>\n",
    "    \"\"\"\n",
    "    context_words_matrix = []\n",
    "    centered_words = tokenized_paragraph\n",
    "    \n",
    "    m = len(tokenized_paragraph)\n",
    "    for idx, word in enumerate(centered_words):\n",
    "        context_words = []\n",
    "        \n",
    "        # Context words before centered word\n",
    "        if idx < C and idx != 0:\n",
    "            context_words += tokenized_paragraph[:idx]\n",
    "        else:\n",
    "            context_words += tokenized_paragraph[idx-C:idx]\n",
    "            \n",
    "        # Context words after centered word\n",
    "        if idx > m-C and idx != m-1:\n",
    "            context_words += tokenized_paragraph[idx:]\n",
    "        else:\n",
    "            context_words += tokenized_paragraph[idx+1:idx+C+1]\n",
    "            \n",
    "        context_words_matrix.append(context_words)\n",
    "    \n",
    "    return context_words_matrix, centered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "intellectual-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(tokenized_paragraphs_df, word_to_index, C, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraphs_df: <Pandas DataFrame>\n",
    "    \n",
    "    word_to_index: dictionary where keys are words and values are indices of the word in the corpus\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    V: <int>\n",
    "        - Size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    XY: <Pandas DataFrame>\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx, row in tokenized_paragraphs_df.iterrows():\n",
    "        paragraph = row[tokenized_paragraphs_df.columns[0]]\n",
    "        context_words_matrix, centered_words = get_context_centered_words(paragraph, C)\n",
    "        \n",
    "        for idx, context_words in enumerate(context_words_matrix):\n",
    "            Y.append(get_one_hot_vector(centered_words[idx], word_to_index, V))\n",
    "            X.append(get_one_hot_from_context_words(context_words, word_to_index, V))\n",
    "    \n",
    "    return pd.DataFrame({'X': X, 'Y': Y})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "threatened-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_X_Y(tokenized_df.iloc[:5, :], word_to_index, C, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ecological-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (139, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                   Y  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Data shape: {data_df.shape}\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-acting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
