{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fifth-desktop",
   "metadata": {},
   "source": [
    "# Práctica 2 - Word2Vec\n",
    "\n",
    "- Martínez Ostoa Néstor I.\n",
    "- Procesamiento de Lenguaje Natural\n",
    "- IeC - FI - UNAM\n",
    "\n",
    "--- \n",
    "**Objetivo**: A partir del corpus seleccionado en el notebook anterior **lab-1-bpe-algorithm.ipynb** realizar un modelo de embeddings basado en Word2Vec. \n",
    "\n",
    "Pasos a realizar: \n",
    "\n",
    "1. Trabajar con el corpus tokenizado\n",
    "2. Obtener los pares de entrenamiento a partir de los contextos\n",
    "3. Construir una red neuronal con una capa con 128 unidades ocultas. Entrenar la red para obtener los embeddings\n",
    "4. Evaluar el modelo (capa de salida) con Entropía o Perplejidad\n",
    "5. Visualizar los embeddings\n",
    "6. Guardar los vectores de la capa de embedding asociados a las palabras\n",
    "\n",
    "---\n",
    "\n",
    "**Corpus elegido:** Don't Patronize Me! dataset ([link](https://github.com/Perez-AlmendrosC/dontpatronizeme))\n",
    "\n",
    "- Este corpus contiene $10,468$ párrafos extraídos de artículos de noticias con el objetivo principal de realizar un análisis para detectar lenguaje condescendiente (*patronizing and condescending language PCL*) en grupos socialmente vulnerables (refugiados, familias pobres, personas sin casa, etc)\n",
    "- Cada uno de estos párrafos están anotados con etiquetas que indican el tipo de lenguaje PCL que se encuentra en él (si es que está presente). Los párrafos se extrajeron del corpus [News on Web (NOW)](https://www.english-corpora.org/now/)\n",
    "- [Link al paper principal](https://aclanthology.org/2020.coling-main.518/)\n",
    "\n",
    "\n",
    "**Estructura del corpus (original - antes del proceso de limpieza)**\n",
    "\n",
    "- De manera general, el dataset contiene párrafos anotados con una etiqueta con valores entre $0$ y $4$ que indican el nivel de lenguaje PCL presente\n",
    "- Cada instancia del dataset está conformada de la siguiente manera:\n",
    "    - ```<doc-id>```: id del documento dentro del corpus NOW\n",
    "    - ```<keyword>```: término de búsqueda utilizado para extraer textos relacionados con una comunidad en específico\n",
    "    - ```<country-code>```: código de dos letras ISO Alpha-2\n",
    "    - ```<paragraph>```: párrafo perteneciente al ```<keyword>```\n",
    "    - ```<label>```: entero que indica el nivel de PCL presente\n",
    "    \n",
    "**Estructura del corpus actual (después del proceso de limpieza)**\n",
    "\n",
    "- ```paragraph```: párrafo limpio sin stop words, signos de puntuación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-crime",
   "metadata": {},
   "source": [
    "## 0. Bibliotecas requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "central-assets",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nestorivanmo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-victoria",
   "metadata": {},
   "source": [
    "## 1. Tokenización del corpus\n",
    "\n",
    "- Como el corpus ya está limpio, lo único que nos queda por hacer es tokenizarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de párrafos: 1570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>refugees identified ioc possible contenders va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>mention moments highlight illustrate potential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>many celebrities wore blue ribbons support ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>game latest longrunning growing strategyrpg se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8107</th>\n",
       "      <td>amy fischer policy director texasbased immigra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              paragraph\n",
       "4007  refugees identified ioc possible contenders va...\n",
       "9503  mention moments highlight illustrate potential...\n",
       "6994  many celebrities wore blue ribbons support ame...\n",
       "3629  game latest longrunning growing strategyrpg se...\n",
       "8107  amy fischer policy director texasbased immigra..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dontpatronizeme_v1.4/dontpatronizeme_pcl_clean.tsv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.sample(frac=0.15, random_state=0)\n",
    "print(f\"Número de párrafos: {df.shape[0]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "increased-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[refugees, identified, ioc, possible, contende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mention, moments, highlight, illustrate, pote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[many, celebrities, wore, blue, ribbons, suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[game, latest, longrunning, growing, strategyr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[amy, fischer, policy, director, texasbased, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tokenized_paragraph\n",
       "0  [refugees, identified, ioc, possible, contende...\n",
       "1  [mention, moments, highlight, illustrate, pote...\n",
       "2  [many, celebrities, wore, blue, ribbons, suppo...\n",
       "3  [game, latest, longrunning, growing, strategyr...\n",
       "4  [amy, fischer, policy, director, texasbased, i..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "for idx, row in df.iterrows():\n",
    "    tdata = nltk.word_tokenize(row[\"paragraph\"])\n",
    "    tokenized_data.append(tdata)\n",
    "    \n",
    "tokenized_df = pd.DataFrame({\"tokenized_paragraph\": tokenized_data})\n",
    "tokenized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signal-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "refugees identified ioc possible contenders various sports selection made june un refugee agency source told afp \n",
      "\n",
      "Tokenized data:\n",
      "['refugees', 'identified', 'ioc', 'possible', 'contenders', 'various', 'sports', 'selection', 'made', 'june', 'un', 'refugee', 'agency', 'source', 'told', 'afp']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original data:\\n{df.iloc[0,0]} \\n\")\n",
    "print(f\"Tokenized data:\\n{tokenized_df.iloc[0,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-above",
   "metadata": {},
   "source": [
    "## 2. Obtención de pares de entrenamiento a partir de los contextos\n",
    "\n",
    "**Entrada**: \n",
    "- DataFrame con los párrafos tokenizados\n",
    "\n",
    "**Salida**: \n",
    "- $X$: matriz de $V\\times m$ con los vectores de las palabras de contexto: ```<Pandas DataFrame>```\n",
    "- $Y$: matriz de $V \\times m$ con los vectores de las palabras centradas: ```<Pandas DataFrame>```\n",
    "\n",
    "donde $V$ es el tamaño del vocabulario de palabras del corpus y $m$ es el tamaño de la ventana y se define como $m=2c + 1$\n",
    "\n",
    "---\n",
    "**Proceso**:\n",
    "\n",
    "1. **Definir $C$**\n",
    "2. **Obtener un vocabulario del corpus**\n",
    "3. **Para cada párrafo tokenizado**:\n",
    "    - *Obtener el vector de palabras de contexto*:\n",
    "        - Con base en $C$, obtener una lista de palabras de contexto\n",
    "        - Para cada palabra de contexto, obtener su codificación *one-hot*\n",
    "        - Hacer el promedio de cada uno de los vectores de contexto\n",
    "    - *Obtener el vector de palabra de centrado*:\n",
    "        - Realizar la codificación *one-hot*\n",
    "    - *Almacenar ambos vectores en dos matrices: $X$ y $Y$*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-symbol",
   "metadata": {},
   "source": [
    "### $C$ - Tamaño del contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "played-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-generator",
   "metadata": {},
   "source": [
    "### Vocabulario del corpus\n",
    "\n",
    "A parte del vocabulario del corpus, obtendremos dos diccionarios útiles:\n",
    "\n",
    "1. ```word_to_index```:\n",
    "    - **Llave**: palabra del corpus\n",
    "    - **Valor**: índice numérico dentro del corpus\n",
    "    \n",
    "2. ```index_to_word```: \n",
    "    - **Llave**: índice numérico de la palabra dentro del corpus\n",
    "    - **Valor**: palabra del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "natural-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vocab(tokenized_data):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_data: <Pandas Dataframe>\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    word_vocab: <set>\n",
    "    \n",
    "    N: <int>\n",
    "        - Size of the word vocabulary\n",
    "    \"\"\"\n",
    "    word_vocab = set()\n",
    "    for _, row in tokenized_data.iterrows():\n",
    "        tokenized_paragraph = row[tokenized_data.columns[0]]\n",
    "        \n",
    "        for word in tokenized_paragraph:\n",
    "            word_vocab.add(word)\n",
    "    \n",
    "    return word_vocab, len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "central-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab, V = get_word_vocab(tokenized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "residential-salmon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:\n",
      "- 10672\n",
      "\n",
      "Vocabulary sample:\n",
      "- lab\n",
      "- recently\n",
      "- gleaming\n",
      "- species\n",
      "- less\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size:\\n- {V}\\n\")\n",
    "print(f\"Vocabulary sample:\")\n",
    "for w in list(word_vocab)[130:135]: print(f\"- {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suffering-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionaries(word_vocab):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word_vocab: <set>\n",
    "        - Contains all the words in the corpus\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "        \n",
    "    index_to_word: <dictionary>\n",
    "        - Key: index of the word in the corpus\n",
    "        - Value: word\n",
    "    \"\"\"\n",
    "    word_to_index = dict()\n",
    "    index_to_word = dict()\n",
    "    \n",
    "    words = sorted(list(word_vocab))\n",
    "    for idx, word in enumerate(words):\n",
    "        index_to_word[idx] = word\n",
    "        word_to_index[word] = idx\n",
    "        \n",
    "    return word_to_index, index_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ruled-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = get_dictionaries(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-knitting",
   "metadata": {},
   "source": [
    "### Obtención de $X$ y $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "comprehensive-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_vector(word, word_to_index, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    word: <str>\n",
    "    \n",
    "    word_to_index: <dictionary>\n",
    "        - Key: word\n",
    "        - Value: index of the word in the corpus\n",
    "    \n",
    "    V: <int>\n",
    "        - size of the corpus' vocabulary of words\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    one_hot_vector: <Numpy's ndarray>\n",
    "    \"\"\"\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word_to_index[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "\n",
    "def get_one_hot_from_context_words(context_words, word_to_index, V):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    context_words: <list>\n",
    "\n",
    "    word_to_index: <dictionary>\n",
    "    \n",
    "    V: <int>\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    one_hot_vector: <numpy's ndarray>\n",
    "        - Mean representation of all the context words' one hot vectors\n",
    "    \"\"\"\n",
    "    one_hot_vectors = [get_one_hot_vector(w, word_to_index, V) for w in context_words]\n",
    "    return np.mean(one_hot_vectors, axis=0)\n",
    "    \n",
    "\n",
    "def get_context_centered_words(tokenized_paragraph, C):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraph: <list>\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    context_words: <list>\n",
    "    \n",
    "    centered_words: <matrix>\n",
    "    \"\"\"\n",
    "    context_words_matrix = []\n",
    "    centered_words = tokenized_paragraph\n",
    "    \n",
    "    m = len(tokenized_paragraph)\n",
    "    for idx, word in enumerate(centered_words):\n",
    "        context_words = []\n",
    "        \n",
    "        # Context words before centered word\n",
    "        if idx < C and idx != 0: context_words += tokenized_paragraph[:idx]\n",
    "        else:                    context_words += tokenized_paragraph[idx-C:idx]\n",
    "            \n",
    "        # Context words after centered word\n",
    "        if idx > m-C and idx != m-1: context_words += tokenized_paragraph[idx:]\n",
    "        else:                        context_words += tokenized_paragraph[idx+1:idx+C+1]\n",
    "            \n",
    "        context_words_matrix.append(context_words)\n",
    "    \n",
    "    return context_words_matrix, centered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intellectual-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y(tokenized_paragraphs_df, word_to_index, V, C):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    -------\n",
    "    tokenized_paragraphs_df: <Pandas DataFrame>\n",
    "    \n",
    "    word_to_index: dictionary where keys are words and values are indices of the word in the corpus\n",
    "    \n",
    "    C: <int>\n",
    "        - Size of the context\n",
    "    \n",
    "    V: <int>\n",
    "        - Size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    XY: <Pandas DataFrame>\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    centered_words_list = []\n",
    "    context_words_list = []\n",
    "    for _, row in tokenized_paragraphs_df.iterrows():\n",
    "        paragraph = row[tokenized_paragraphs_df.columns[0]]\n",
    "        context_words_matrix, centered_words = get_context_centered_words(paragraph, C)\n",
    "        \n",
    "        for idx, context_words in enumerate(context_words_matrix):\n",
    "            centered_words_list.append(centered_words[idx])\n",
    "            Y.append(\n",
    "                get_one_hot_vector(centered_words[idx], word_to_index, V)\n",
    "            )\n",
    "            \n",
    "            context_words_list.append(context_words)\n",
    "            X.append(\n",
    "                get_one_hot_from_context_words(context_words, word_to_index, V)\n",
    "            )\n",
    "    \n",
    "    df_dict = {\n",
    "        \"centered_word\": np.array(centered_words_list),\n",
    "        \"context_words\": np.array(context_words_list),\n",
    "        \"X\": X,\n",
    "        \"Y\": Y,\n",
    "    }\n",
    "    XY = pd.DataFrame(df_dict)\n",
    "    return XY\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "satisfied-abraham",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-12-310b6299f247>:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"context_words\": np.array(context_words_list),\n"
     ]
    }
   ],
   "source": [
    "data_df = get_X_Y(tokenized_df, word_to_index, V, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "experienced-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1570, 1)\n",
      "(38069, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>centered_word</th>\n",
       "      <th>context_words</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refugees</td>\n",
       "      <td>[identified, ioc]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>identified</td>\n",
       "      <td>[refugees, ioc, possible]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ioc</td>\n",
       "      <td>[refugees, identified, possible, contenders]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>possible</td>\n",
       "      <td>[identified, ioc, contenders, various]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contenders</td>\n",
       "      <td>[ioc, possible, various, sports]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  centered_word                                 context_words  \\\n",
       "0      refugees                             [identified, ioc]   \n",
       "1    identified                     [refugees, ioc, possible]   \n",
       "2           ioc  [refugees, identified, possible, contenders]   \n",
       "3      possible        [identified, ioc, contenders, various]   \n",
       "4    contenders              [ioc, possible, various, sports]   \n",
       "\n",
       "                                                   X  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                   Y  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_df.shape)\n",
    "print(data_df.shape)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amino-europe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10672,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.iloc[1, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "detailed-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 38069 x 10672\n",
      "Y shape: 38069 x 10672\n"
     ]
    }
   ],
   "source": [
    "X = data_df['X']\n",
    "Y = data_df['Y']\n",
    "\n",
    "print(f\"X shape: {X.shape[0]} x {X.iloc[0].shape[0]}\")\n",
    "print(f\"Y shape: {Y.shape[0]} x {Y.iloc[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-poetry",
   "metadata": {},
   "source": [
    "## 3. Red Neuronal\n",
    "\n",
    "**Instrucción**: construir una red neuronal cocn 128 unidades ocultas. Entrenar la red para obtener los embeddings. \n",
    "\n",
    "**Desarrollo**: en esta práctica estaremos utilizando la arquitectura neuronal **word2vec** para genear embeddings de palabras. Concretamente, estaremos utilizando **CBOW (continuous bag-of-words)** como diseño de arquitectura neuronal en la cual notaremos lo siguiente: \n",
    "\n",
    "- La entrada a esta red neuronal serán los vectores de palabras de contexto $X$\n",
    "- La salida de esta red neuronal será el vector de palabras de centrado estimado $\\hat{y}$\n",
    "\n",
    "---\n",
    "\n",
    "**Arquitectura neuronal - CBOW**\n",
    "\n",
    "- **Capa de entrada $X$:**\n",
    "    - Matriz de vectores de palabras de contexto $X$\n",
    "    - $X$ es de dimensiones $V\\times m$\n",
    "        - $V=38069$\n",
    "        - $m=10672$\n",
    "- **Capa oculta $H$:**\n",
    "    - $H$ es de dimensiones $N\\times m$\n",
    "        - $N = 128$ \n",
    "    - $H = \\text{ReLU}(Z_1)$\n",
    "    - $Z_1 = W_1X + B_1 $\n",
    "- **Capa de salida $\\hat{y}$:**\n",
    "    - $\\hat{y}$ es de dimensiones $V \\times m$\n",
    "    - $\\hat{y} = \\text{softmax}(Z_2)$\n",
    "    - $Z_2 = W_2H + B_2$\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Funciones de activación\n",
    "\n",
    "Para implementar esta red neuronal, utilizamos dos funciones de activación:\n",
    "\n",
    "1. **Rectified Linear Unit (ReLU)**:\n",
    "    - $\\text{ReLU}(z) = \\max(0, z)$\n",
    "2. **Softmax**\n",
    "    - $\\hat{y} = \\frac{\\exp(z)}{\\sum_{j=1}^V \\exp(z_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "legitimate-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de activación\n",
    "def ReLU(z):\n",
    "    result = z.copy()\n",
    "    result[result < 0] = 0    \n",
    "    return result\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-violin",
   "metadata": {},
   "source": [
    "### 3.2 Forward propagation\n",
    "\n",
    "1. Definición del hiperparámetro $N$\n",
    "2. Inicialización de las matrices $W_1, W_2, B_1, B_2$\n",
    "3. Funciones para calcular $Z_1, H$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "musical-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "humanitarian-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_biases(N, V, random_state=0):\n",
    "    W1 = np.random.rand(N,V)\n",
    "    B1 = np.random.rand(N,1)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    B2 = np.random.rand(V,1)\n",
    "    return W1, B1, W2, B2\n",
    "    \n",
    "\n",
    "def forward_propagation(X_i, W1, B1, W2, B2):\n",
    "    X_i = X_i.reshape((X_i.shape[0], 1))\n",
    "    H = ReLU(W1@X_i + B1)\n",
    "    Z = np.dot(W2, H) + B2\n",
    "    return H, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "documentary-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 128\tV: 10672\n",
      "W1 shape: (128, 10672)\n",
      "B1 shape: (128, 1)\n",
      "W2 shape: (10672, 128)\n",
      "B2 shape: (10672, 1)\n",
      "H shape: (128, 1)\n",
      "Z shape: (10672, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"N: {N}\\tV: {V}\")\n",
    "W1, B1, W2, B2 = init_weights_biases(N, V)\n",
    "print(f\"W1 shape: {W1.shape}\")\n",
    "print(f\"B1 shape: {B1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape}\")\n",
    "print(f\"B2 shape: {B2.shape}\")\n",
    "\n",
    "H, Z = forward_propagation(X.iloc[0], W1, B1, W2, B2)\n",
    "print(f\"H shape: {H.shape}\")\n",
    "print(f\"Z shape: {Z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-shape",
   "metadata": {},
   "source": [
    "### 3.3 Función de costo\n",
    "\n",
    "Para esta red neuronal utilizaremos la función pérdida entropía cruzada la cual se define como: \n",
    "\n",
    "$$ J=-\\sum_{k=1}^{V}y_k\\log{\\hat{y}_k} \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "invalid-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_predicted, y_actual):\n",
    "    return np.sum(-np.log(y_predicted)*y_actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-flashing",
   "metadata": {},
   "source": [
    "### 3.4 Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X_i, y, y_hat, H, W1, B1, W2, B2, batch_size):\n",
    "    grad_W1 = (1/batch_size) * ReLU((W2.T@(y_hat - y))@X_i.T)\n",
    "    grad_W2 = (1/batch_size) * (y_hat - y)@H.T\n",
    "    grad_b1 = np.sum((1/batch_size) * )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-dylan",
   "metadata": {},
   "source": [
    "### 3.5 Gradient Desccent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-sector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "geographic-philippines",
   "metadata": {},
   "source": [
    "## 4. Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-walnut",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vocal-liechtenstein",
   "metadata": {},
   "source": [
    "## 5. Visualización de los Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-vienna",
   "metadata": {},
   "source": [
    "## 6. Almacenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sound-catalog",
   "metadata": {},
   "source": [
    "**Referencias**:\n",
    "\n",
    "\n",
    "- [V. Mijangos - Curso Procesamiento de Lenguaje Natural](https://github.com/VMijangos/Curso-Procesamiento-de-Lenguaje-Natural/tree/master/Notebooks)\n",
    "- [ElotlMX - Curso Redes Neuronales](https://github.com/ElotlMX/Curso_redes)\n",
    "- [Deep Learning AI NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-scottish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
